
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Exploratory data analysis of Iowa liquor sales using the BigQuery DataFrames package</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14" ga4id=""></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  codelab-ga4id=""
                  id="bigquery-dataframes-iowa-liquor-sales"
                  title="Exploratory data analysis of Iowa liquor sales using the BigQuery DataFrames package"
                  environment="web"
                  feedback-link="">
    
      <google-codelab-step label="Overview" duration="0">
        <p>In this lab, you will use BigQuery DataFrames from a Python notebook in BigQuery Studio to clean and analyze the Iowa liquor sales public dataset. Make use of BigQuery ML and remote function capabilities to discover insights.</p>
<p>You will create a Python notebook to compare sales across geographic areas. This can be adapted to work on any structured data.</p>
<h2 is-upgraded>Objectives</h2>
<p>In this lab, you learn how to perform the following tasks:</p>
<ul>
<li>Activate and use Python notebooks in BigQuery Studio</li>
<li>Connect to BigQuery using the BigQuery DataFrames package</li>
<li>Create a linear regression using BigQuery ML</li>
<li>Perform complex aggregations and joins using a familiar pandas-like syntax</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Requirements" duration="0">
        <ul>
<li>A browser, such as  <a href="https://www.google.com/chrome/browser/desktop/" target="_blank">Chrome</a> or  <a href="https://www.mozilla.org/firefox/" target="_blank">Firefox</a></li>
<li>A Google Cloud project with billing enabled</li>
</ul>
<h2 is-upgraded>Before you begin</h2>
<p>To follow the instructions in this codelab, you&#39;ll need a Google Cloud Project with BigQuery Studio enabled and a connected billing account.</p>
<ol type="1">
<li>In the  <a href="https://console.cloud.google.com/" target="_blank">Google Cloud Console</a>, on the project selector page, select or create a Google Cloud <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects" target="_blank">project</a></li>
<li>Ensure that billing is enabled for your Google Cloud project. Learn how to  <a href="https://cloud.google.com/billing/docs/how-to/verify-billing-enabled" target="_blank">check if billing is enabled on a project</a></li>
<li>Follow the instructions to <a href="https://cloud.google.com/bigquery/docs/enable-assets" target="_blank">Enable BigQuery Studio for asset management</a>.</li>
</ol>
<h2 is-upgraded>Prepare BigQuery Studio</h2>
<p>Create an empty notebook and connect it to a runtime.</p>
<ol type="1">
<li>Go to <a href="https://console.cloud.google.com/bigquery" target="_blank">BigQuery Studio</a> in the Google Cloud Console.</li>
<li>Click the <strong>â–¼</strong> next to the <strong>+</strong> button.</li>
<li>Select <strong>Python notebook</strong>.</li>
<li>Close the template selector.</li>
<li>Select <strong>+ Code</strong> to create a new code cell.</li>
<li>Install the latest version of the BigQuery DataFrames package from the code cell.Type the following command.<pre><code>%pip install --upgrade bigframes --quiet
</code></pre>
Click the <strong>ðŸž‚</strong> button or press <em>Shift + Enter</em> to run the code cell.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Read a public dataset" duration="0">
        <p>Initialize the BigQuery DataFrames package by running the following in a new code cell:</p>
<pre><code language="language-python" class="language-python">import bigframes.pandas as bpd

bpd.options.bigquery.ordering_mode = &#34;partial&#34;
bpd.options.display.repr_mode = &#34;deferred&#34;
</code></pre>
<p>Note: in this tutorial, we use the experimental &#34;partial ordering mode&#34;, which allows for more efficient queries when used with pandas-like filtering. Some pandas features that require a strict ordering or index may not work.</p>
<p>Check your <code>bigframes</code> package version with</p>
<pre><code>bpd.__version__
</code></pre>
<p>This tutorial requires version 1.27.0 or later.</p>
<h2 is-upgraded>Iowa liquor retail sales</h2>
<p>The <a href="https://console.cloud.google.com/marketplace/product/iowa-department-of-commerce/iowa-liquor-sales" target="_blank">Iowa liquor retail sales dataset</a> is provided on BigQuery through <a href="https://cloud.google.com/datasets" target="_blank">Google Cloud&#39;s public dataset program</a>. This dataset contains every wholesale purchase of liquor in the State of Iowa by retailers for sale to individuals since January 1, 2012. Data are collected by the Alcoholic Beverages Division within the Iowa Department of Commerce.</p>
<p>In BigQuery, query the <a href="https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=iowa_liquor_sales&t=sales&page=table" target="_blank">bigquery-public-data.iowa_liquor_sales.sales</a>. to analyze the Iowa liquor retail sales. Use the <code>bigframes.pandas.read_gbq()</code> method to create a DataFrame from a query string or table ID.</p>
<p>Run the following in a new code cell to create a DataFrame named &#34;df&#34;:</p>
<pre><code>df = bpd.read_gbq_table(&#34;bigquery-public-data.iowa_liquor_sales.sales&#34;)
</code></pre>
<h2 is-upgraded>Discover basic information about a DataFrame</h2>
<p>Use the <code>DataFrame.peek()</code> method to download a small sample of the data.</p>
<p><strong>Run this cell:</strong></p>
<pre><code>df.peek()
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>index	invoice_and_item_number	date	store_number	store_name	...
0	RINV-04620300080	2023-04-28	10197	SUNSHINE FOODS / HAWARDEN	
1	RINV-04864800097	2023-09-25	2621	HY-VEE FOOD STORE #3 / SIOUX CITY	
2	RINV-05057200028	2023-12-28	4255	FAREWAY STORES #058 / ORANGE CITY	
3	...				
</code></pre>
<p>Note: <code>head()</code> requires ordering and is generally less efficient than <code>peek()</code> if you want to visualize a sample of data.</p>
<p>Just as with pandas, use the <code>DataFrame.dtypes</code> property to see all available columns and their corresponding data types. These are exposed in a pandas-compatible way.</p>
<p><strong>Run this cell:</strong></p>
<pre><code>df.dtypes
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>invoice_and_item_number	string[pyarrow]
date	date32[day][pyarrow]
store_number	string[pyarrow]
store_name	string[pyarrow]
address	string[pyarrow]
city	string[pyarrow]
zip_code	string[pyarrow]
store_location	geometry
county_number	string[pyarrow]
county	string[pyarrow]
category	string[pyarrow]
category_name	string[pyarrow]
vendor_number	string[pyarrow]
vendor_name	string[pyarrow]
item_number	string[pyarrow]
item_description	string[pyarrow]
pack	Int64
bottle_volume_ml	Int64
state_bottle_cost	Float64
state_bottle_retail	Float64
bottles_sold	Int64
sale_dollars	Float64
volume_sold_liters	Float64
volume_sold_gallons	Float64

dtype: object
</code></pre>
<p>The <code>DataFrame.describe()</code> method queries some basic statistics from the DataFrame. Run <code>DataFrame.to_pandas()</code> to download these summary statistics as a pandas DataFrame.</p>
<p><strong>Run this cell:</strong></p>
<pre><code>df.describe(&#34;all&#34;).to_pandas()
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>	invoice_and_item_number	date	store_number	store_name	...
nunique	30305765	&lt;NA&gt;	3158	3353	...
std	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	...
mean	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	...
75%	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	...
25%	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	...
count	30305765	&lt;NA&gt;	30305765	30305765	...
min	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	...
50%	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	...
max	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	&lt;NA&gt;	...
9 rows Ã— 24 columns
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Visualize and clean the data" duration="0">
        <p>The Iowa liquor retail sales dataset provides fine-grained geographic information, including where the retail stores are located. Use these data to identify trends and differences across geographic areas.</p>
<h2 is-upgraded>Visualize sales per zip code</h2>
<p>There are several built-in visualization methods such as <a href="https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.operations.plotting.PlotAccessor#bigframes_operations_plotting_PlotAccessor_hist" target="_blank">DataFrame.plot.hist()</a>. Use this method to compare liquor sales by ZIP code.</p>
<pre><code>volume_by_zip = df.groupby(&#34;zip_code&#34;).agg({&#34;volume_sold_liters&#34;: &#34;sum&#34;})
volume_by_zip.plot.hist(bins=20)
</code></pre>
<p><strong>Expected output:</strong></p>
<p class="image-container"><img alt="Histogram of volumes" src="img/ecb69bf743882ffc.png"></p>
<p>Use a bar chart to see which zip colds sold the most alcohol.</p>
<pre><code>(
  volume_by_zip
  .sort_values(&#34;volume_sold_liters&#34;, ascending=False)
  .head(25)
  .to_pandas()
  .plot.bar(rot=80)
)
</code></pre>
<p><strong>Expected output:</strong></p>
<p class="image-container"><img alt="Bar chart of volumes of alcohol in the top selling zip codes" src="img/d92f1ec3cc48f573.png"></p>
<h2 is-upgraded>Clean the data</h2>
<p>Some ZIP codes have a trailing <code>.0</code>. Possibly somewhere in the data collection the ZIP codes were accidentally converted into floating point values. Use regular expressions to clean up the ZIP codes and repeat the analysis.</p>
<pre><code>df = (
    bpd.read_gbq_table(&#34;bigquery-public-data.iowa_liquor_sales.sales&#34;)
    .assign(
        zip_code=lambda _: _[&#34;zip_code&#34;].str.replace(&#34;.0&#34;, &#34;&#34;)
    )
)
volume_by_zip = df.groupby(&#34;zip_code&#34;).agg({&#34;volume_sold_liters&#34;: &#34;sum&#34;})
(
  volume_by_zip
  .sort_values(&#34;volume_sold_liters&#34;, ascending=False)
  .head(25)
  .to_pandas()
  .plot.bar(rot=80)
)
</code></pre>
<p><strong>Expected output:</strong></p>
<p class="image-container"><img alt="Bar chart of volumes of alcohol in the top selling zip codes" src="img/ffbc3ded9a77c3ca.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Discover correlations in sales" duration="0">
        <p>Why do some zip codes sell more than others? One hypothesis is that it&#39;s due to population size differences. A zip code with more population will likely sell more liquor.</p>
<p>Test this hypothesis by calculating the correlation between population and liquor sales volume.</p>
<h2 is-upgraded>Join with other datasets</h2>
<p>Join with a population dataset such as the <a href="https://console.cloud.google.com/marketplace/product/united-states-census-bureau/acs" target="_blank">US Census Bureau&#39;s American Community Survey</a> ZIP code tabulation area survey.</p>
<pre><code>census_acs = bpd.read_gbq_table(&#34;bigquery-public-data.census_bureau_acs.zcta_2020_5yr&#34;)
</code></pre>
<p>The American Community Survey identifies states by GEOID. In the case of ZIP code tabulation areas, the GEOID equals the ZIP code.</p>
<pre><code>volume_by_pop = volume_by_zip.join(
    census_acs.set_index(&#34;geo_id&#34;)
)
</code></pre>
<p>Create a scatter plot to compare ZIP code tabulation area populations with liters of alcohol sold.</p>
<pre><code>(
    volume_by_pop[[&#34;volume_sold_liters&#34;, &#34;total_pop&#34;]]
    .to_pandas()
    .plot.scatter(x=&#34;total_pop&#34;, y=&#34;volume_sold_liters&#34;)
)
</code></pre>
<p><strong>Expected output:</strong></p>
<p class="image-container"><img alt="Scatter plot of zip code tabulation areas by the population and liters of liquor sold" src="img/2790822216505a89.png"></p>
<h2 is-upgraded>Calculate correlations</h2>
<p>The trend looks roughly linear. Fit a linear regression model to this to check how well population can predict liquor sales.</p>
<pre><code>from bigframes.ml.linear_model import LinearRegression

feature_columns = volume_by_pop[[&#34;total_pop&#34;]]
label_columns = volume_by_pop[[&#34;volume_sold_liters&#34;]]

# Create the linear model
model = LinearRegression()
model.fit(feature_columns, label_columns)
</code></pre>
<p>Check how good the fit is by using the <code>score</code> method.</p>
<pre><code>model.score(feature_columns, label_columns).to_pandas()
</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>	mean_absolute_error	mean_squared_error	mean_squared_log_error	median_absolute_error	r2_score	explained_variance
0	245065.664095	224398167097.364288	5.595021	178196.31289	0.380096	0.380096
</code></pre>
<p>Draw the best fit line but calling the <code>predict</code> function on a range of population values.</p>
<pre><code>import matplotlib.pyplot as pyplot
import numpy as np
import pandas as pd

line = pd.Series(np.arange(0, 50_000), name=&#34;total_pop&#34;)
predictions = model.predict(line).to_pandas()

zips = volume_by_pop[[&#34;volume_sold_liters&#34;, &#34;total_pop&#34;]].to_pandas()
pyplot.scatter(zips[&#34;total_pop&#34;], zips[&#34;volume_sold_liters&#34;])
pyplot.plot(
  line,
  predictions.sort_values(&#34;total_pop&#34;)[&#34;predicted_volume_sold_liters&#34;],
  marker=None,
  color=&#34;red&#34;,
)
</code></pre>
<p><strong>Expected output:</strong></p>
<p class="image-container"><img alt="Scatter plot with a best fit line" src="img/79d55100dc344f49.png"></p>
<h2 is-upgraded>Addressing heteroscedasticity</h2>
<p>The data in the previous chart appears to be heteroscedastic. The variance around the best fit line grows with the population.</p>
<p>Perhaps the amount of alcohol purchased per person is relatively constant.</p>
<pre><code>volume_per_pop = (
    volume_by_pop[volume_by_pop[&#39;total_pop&#39;] &gt; 0]
    .assign(liters_per_pop=lambda df: df[&#34;volume_sold_liters&#34;] / df[&#34;total_pop&#34;])
)

(
    volume_per_pop[[&#34;liters_per_pop&#34;, &#34;total_pop&#34;]]
    .to_pandas()
    .plot.scatter(x=&#34;total_pop&#34;, y=&#34;liters_per_pop&#34;)
)
</code></pre>
<p><strong>Expected output:</strong></p>
<p class="image-container"><img alt="Scatter plot of liters per population" src="img/87f6db2eb7f1def0.png"></p>
<p>Calculate the average liters of alcohol purchased in two different ways:</p>
<ol type="1">
<li>What is the average amount of alcohol purchased per person in Iowa?</li>
<li>What is the average over all zip codes of the amount of alcohol purchased per person.</li>
</ol>
<p>In (1), it reflects how much alcohol is purchased in the whole state. In (2), it reflects the average zip code, which won&#39;t necessarily be the same as (1) because different zip codes have different populations.</p>
<pre><code>df = bpd.read_gbq_table(&#34;bigquery-public-data.iowa_liquor_sales.sales&#34;)
census_state = bpd.read_gbq(
    &#34;bigquery-public-data.census_bureau_acs.state_2020_5yr&#34;,
    index_col=&#34;geo_id&#34;,
)

volume_per_pop_statewide = (
    df[&#39;volume_sold_liters&#39;].sum()
    / census_state[&#34;total_pop&#34;].loc[&#39;19&#39;]
)
volume_per_pop_statewide
</code></pre>
<p><strong>Expected output: </strong><code>87.997</code></p>
<pre><code>average_per_zip = volume_per_pop[&#34;liters_per_pop&#34;].mean()
average_per_zip
</code></pre>
<p><strong>Expected output: </strong><code>37.468</code></p>
<p>Plot these averages, similar to above.</p>
<pre><code>import numpy as np
import pandas as pd
from matplotlib import pyplot

line = pd.Series(np.arange(0, 50_000), name=&#34;total_pop&#34;)

zips = volume_per_pop[[&#34;liters_per_pop&#34;, &#34;total_pop&#34;]].to_pandas()
pyplot.scatter(zips[&#34;total_pop&#34;], zips[&#34;liters_per_pop&#34;])
pyplot.plot(line, np.full(line.shape, volume_per_pop_statewide), marker=None, color=&#34;magenta&#34;)
pyplot.plot(line, np.full(line.shape, average_per_zip), marker=None, color=&#34;red&#34;)
</code></pre>
<p><strong>Expected output:</strong></p>
<p class="image-container"><img alt="Scatter plot of liters per population" src="img/1d60a89e2b287389.png"></p>
<p>There are still some zip codes that are quite large outliers, especially in areas with less population. It is left as an exercise to hypothesize why this is. For example, it could be that some zip codes are low population but high consumption because they contain the only liquor store in the area. If so, calculating based the population of surrounding zip codes may even these outliers out.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Comparing types of liquor sold" duration="0">
        <p>In addition to geographic data, the Iowa liquor retail sales database also contains detailed information about the item sold. Perhaps by analyzing these, we can reveal differences in tastes across geographic areas.</p>
<h2 is-upgraded>Explore categories</h2>
<p>Items are categorized in the database. How many categories are there?</p>
<pre><code>import bigframes.pandas as bpd

bpd.options.bigquery.ordering_mode = &#34;partial&#34;
bpd.options.display.repr_mode = &#34;deferred&#34;

df = bpd.read_gbq_table(&#34;bigquery-public-data.iowa_liquor_sales.sales&#34;)
df.category_name.nunique()
</code></pre>
<p><strong>Expected output: </strong><code>103</code></p>
<p>Which are the most popular categories by volume?</p>
<pre><code>counts = (
    df.groupby(&#34;category_name&#34;)
    .agg({&#34;volume_sold_liters&#34;: &#34;sum&#34;})
    .sort_values([&#34;volume_sold_liters&#34;], ascending=False)
    .to_pandas()
)
counts.head(25).plot.bar(rot=80)
</code></pre>
<p class="image-container"><img alt="Bar chart of top categories of liquor sold" src="img/7310d67ff623834.png"></p>
<h2 is-upgraded>Working with the ARRAY data type</h2>
<p>There are several categories each of whiskey, rum, vodka, and more. I&#39;d like to group these together somehow.</p>
<p>Start by splitting the category names into separate words by using the <a href="https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.operations.strings.StringMethods#bigframes_operations_strings_StringMethods_split" target="_blank">Series.str.split()</a> method. Unnest the array this creates by using the <code>explode()</code> method.</p>
<pre><code>category_parts = df.category_name.str.split(&#34; &#34;).explode()
counts = (
    category_parts
    .groupby(category_parts)
    .size()
    .sort_values(ascending=False)
    .to_pandas()
)
counts.head(25).plot.bar(rot=80)
</code></pre>
<p class="image-container"><img alt="Words by count from categories" src="img/5b50275b871379fd.png"></p>
<pre><code>category_parts.nunique()
</code></pre>
<p><strong>Expected output: </strong><code>113</code></p>
<p>Looking at the chart above, the data still have VODKA separate from VODKAS. More grouping is needed to collapse categories into a smaller set.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Using NLTK with BigQuery DataFrames" duration="0">
        <p>With only about 100 categories, it would be feasible to write some heuristics or even manually create a mapping from category to the wider liquor type. Alternatively, one could use a large language model such as Gemini to create such a mapping. Try the codelab <a href="/bigquery-dataframes-clustering-unstructured-data" target="_blank">Get insights from unstructured data using BigQuery DataFrames</a> to use BigQuery DataFrames with Gemini.</p>
<p>Instead, use a more traditional natural language processing package, NLTK, to process these data. Technology called a &#34;stemmer&#34; can merge plural and singular nouns into the same value, for example.</p>
<h2 is-upgraded>Create a new notebook</h2>
<p>Click the arrow in BigQuery Studio&#39;s tabbed editor and select <strong>Create new Python notebook</strong>.</p>
<h2 is-upgraded>Using NLTK to stem words</h2>
<p>The <a href="https://www.nltk.org/" target="_blank">NLTK package</a> provides natural language processing methods that are accessible from Python. Install the package to try it out.</p>
<pre><code>%pip install nltk
</code></pre>
<p>Next, import the package. Inspect the version. It will be used later on in the tutorial.</p>
<pre><code>import nltk

nltk.__version__
</code></pre>
<p>One way of standardizing words to &#34;stem&#34; the word. This removes any suffixes, as a trailing &#34;s&#34; for plurals.</p>
<pre><code>def stem(word: str) -&gt; str:
    # https://www.nltk.org/howto/stem.html
    import nltk.stem.snowball

    # Avoid failure if a NULL is passed in.
    if not word:
        return word

    stemmer = nltk.stem.snowball.SnowballStemmer(&#34;english&#34;)
    return stemmer.stem(word)
</code></pre>
<p>Try this out on a few words.</p>
<pre><code>stem(&#34;WHISKEY&#34;)
</code></pre>
<p><strong>Expected output: </strong><code>whiskey</code></p>
<pre><code>stem(&#34;WHISKIES&#34;)
</code></pre>
<p><strong>Expected output: </strong><code>whiski</code></p>
<p>Unfortunately, this didn&#39;t map whiskies to the same as whiskey. Stemmers don&#39;t work well with irregular plurals. Try a lemmatizer, which uses more sophisticated techniques to identify the base word, called a &#34;lemma&#34;.</p>
<pre><code>def lemmatize(word: str) -&gt; str:
    # https://stackoverflow.com/a/18400977/101923
    # https://www.nltk.org/api/nltk.stem.wordnet.html#module-nltk.stem.wordnet
    import nltk
    import nltk.stem.wordnet


    # Avoid failure if a NULL is passed in.
    if not word:
        return word

    nltk.download(&#39;wordnet&#39;)
    wnl = nltk.stem.wordnet.WordNetLemmatizer()
    return wnl.lemmatize(word.lower())
</code></pre>
<p>Try this out on a few words.</p>
<pre><code>lemmatize(&#34;WHISKIES&#34;)
</code></pre>
<p><strong>Expected output: </strong><code>whisky</code></p>
<pre><code>lemmatize(&#34;WHISKY&#34;)
</code></pre>
<p><strong>Expected output: </strong><code>whisky</code></p>
<pre><code>lemmatize(&#34;WHISKEY&#34;)
</code></pre>
<p><strong>Expected output: </strong><code>whiskey</code></p>
<p>Unfortunately, this lemmatizer doesn&#39;t map &#34;whiskey&#34; to the same lemma as &#34;whiskies&#34;. Since this word is particularly important for the Iowa retail liquor sales database, manually map it to the American spelling by using a dictionary.</p>
<pre><code>def lemmatize(word: str) -&gt; str:
    # https://stackoverflow.com/a/18400977/101923
    # https://www.nltk.org/api/nltk.stem.wordnet.html#module-nltk.stem.wordnet
    import nltk
    import nltk.stem.wordnet


    # Avoid failure if a NULL is passed in.
    if not word:
        return word

    nltk.download(&#39;wordnet&#39;)
    wnl = nltk.stem.wordnet.WordNetLemmatizer()
    lemma = wnl.lemmatize(word.lower())

    table = {
        &#34;whisky&#34;: &#34;whiskey&#34;,  # Use the American spelling.
    }
    return table.get(lemma, lemma)
</code></pre>
<p>Try this out on a few words.</p>
<pre><code>lemmatize(&#34;WHISKIES&#34;)
</code></pre>
<p><strong>Expected output: </strong><code>whiskey</code></p>
<pre><code>lemmatize(&#34;WHISKEY&#34;)
</code></pre>
<p><strong>Expected output: </strong><code>whiskey</code></p>
<p>Congrats! This lemmatizer should work well for narrowing the categories. To use it with BigQuery, you must deploy it to the cloud.</p>
<h2 is-upgraded>Setup your project for function deployment</h2>
<p>Before you deploy this to the cloud so that BigQuery can access this function, you&#39;ll need to do some one time setup.</p>
<p>Create a new code cell and replace <code>your-project-id</code> with the Google Cloud project ID you&#39;re using for this tutorial.</p>
<pre><code>project_id = &#34;your-project-id&#34;
</code></pre>
<p>Create a service account without any permissions, since this function doesn&#39;t need access to any cloud resources.</p>
<pre><code>from google.cloud import iam_admin_v1
from google.cloud.iam_admin_v1 import types

iam_admin_client = iam_admin_v1.IAMClient()
request = types.CreateServiceAccountRequest()

request.account_id = account_id
request.name = f&#34;projects/{project_id}&#34;

account_id = &#34;bigframes-no-permissions&#34;
display_name = &#34;bigframes remote function (no permissions)&#34;
service_account = types.ServiceAccount()
service_account.display_name = display_name
request.service_account = service_account

account = iam_admin_client.create_service_account(request=request)
print(account.email)
</code></pre>
<p><strong>Expected output: </strong><code>bigframes-no-permissions@your-project-id.iam.gserviceaccount.com</code></p>
<p>Create a BigQuery dataset to hold the function.</p>
<pre><code>from google.cloud import bigquery

bqclient = bigquery.Client(project=project_id)
dataset = bigquery.Dataset(f&#34;{project_id}.functions&#34;)
bqclient.create_dataset(dataset, exists_ok=True)
</code></pre>
<h2 is-upgraded>Deploying a remote function</h2>
<p>Now, deploy your function to the dataset you just created. Add a <code>@bpd.remote_function</code> decorator to the function you created in the previous steps.</p>
<pre><code>import bigframes.pandas as bpd


bpd.options.bigquery.ordering_mode = &#34;partial&#34;
bpd.options.display.repr_mode = &#34;deferred&#34;


@bpd.remote_function(
    dataset=f&#34;{project_id}.functions&#34;,
    name=&#34;lemmatize&#34;,
    # TODO: Replace this with your version of nltk.
    packages=[&#34;nltk==3.9.1&#34;],
    # Replace this with your service account email.
    cloud_function_service_account=&#34;bigframes-no-permissions@your-project-id.iam.gserviceaccount.com&#34;,
    cloud_function_ingress_settings=&#34;internal-only&#34;,
)
def lemmatize(word: str) -&gt; str:
    # https://stackoverflow.com/a/18400977/101923
    # https://www.nltk.org/api/nltk.stem.wordnet.html#module-nltk.stem.wordnet
    import nltk
    import nltk.stem.wordnet


    # Avoid failure if a NULL is passed in.
    if not word:
        return word

    nltk.download(&#39;wordnet&#39;)
    wnl = nltk.stem.wordnet.WordNetLemmatizer()
    lemma = wnl.lemmatize(word.lower())

    table = {
        &#34;whisky&#34;: &#34;whiskey&#34;,  # Use the American spelling.
    }
    return table.get(lemma, lemma)
</code></pre>
<p>Deployment should take about two minutes.</p>
<h2 is-upgraded>Using the remote functions</h2>
<p>Once the deployment completes, you can switch back to your original notebook to test this function.</p>
<pre><code>import bigframes.pandas as bpd

bpd.options.bigquery.ordering_mode = &#34;partial&#34;
bpd.options.display.repr_mode = &#34;deferred&#34;

lemmatize = bpd.read_gbq_function(&#34;swast-scratch.functions.lemmatize&#34;)

words = bpd.Series([&#34;whiskies&#34;, &#34;whisky&#34;, &#34;whiskey&#34;, &#34;vodkas&#34;, &#34;vodka&#34;])
words.apply(lemmatize).to_pandas()
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Comparing alcohol consumption by county" duration="0">
        <p>Now that the <code>lemmatize</code> function is available, use it to combine categories.</p>
<h2 is-upgraded>Finding the word to best summarize the category</h2>
<p>First, create a DataFrame of all categories in the database.</p>
<pre><code>df = bpd.read_gbq_table(&#34;bigquery-public-data.iowa_liquor_sales.sales&#34;)

categories = (
    df[&#39;category_name&#39;]
    .groupby(df[&#39;category_name&#39;])
    .size()
    .to_frame()
    .rename(columns={&#34;category_name&#34;: &#34;total_orders&#34;})
    .reset_index(drop=False)
)
categories.to_pandas()
</code></pre>
<p>Next, create a DataFrame of all words in the categories, except for a few filler words like punctuation and &#34;item&#34;.</p>
<pre><code>words = (
    categories.assign(
        words=categories[&#39;category_name&#39;]
        .str.lower()
        .str.split(&#34; &#34;)
    )
    .assign(num_words=lambda _: _[&#39;words&#39;].str.len())
    .explode(&#34;words&#34;)
    .rename(columns={&#34;words&#34;: &#34;word&#34;})
)
words = words[
    # Remove punctuation and &#34;item&#34;, unless it&#39;s the only word
    (words[&#39;word&#39;].str.isalnum() &amp; ~(words[&#39;word&#39;].str.startswith(&#39;item&#39;)))
    | (words[&#39;num_words&#39;] == 1)
]
words.to_pandas()
</code></pre>
<p>Note that by lemmatizing after grouping, you are reducing the load on your Cloud Function. It is possible to apply the lemmatize function on each of the several million rows in the database, but it would cost more than applying it after grouping and may require quota increases.</p>
<pre><code>lemmas = words.assign(lemma=lambda _: _[&#34;word&#34;].apply(lemmatize))
lemmas.to_pandas()
</code></pre>
<p>Now that the words have been lemmatize, you need to select the lemma that best summarizes the category. Since there aren&#39;t many function words in the categories, use the heuristic that if a word appears in multiple other categories, it&#39;s likely better as a summarizing word (e.g. whiskey).</p>
<pre><code>lemma_counts = (
    lemmas
    .groupby(&#34;lemma&#34;, as_index=False)
    .agg({&#34;total_orders&#34;: &#34;sum&#34;})
    .rename(columns={&#34;total_orders&#34;: &#34;total_orders_with_lemma&#34;})
)

categories_with_lemma_counts = lemmas.merge(lemma_counts, on=&#34;lemma&#34;)

max_lemma_count = (
    categories_with_lemma_counts
    .groupby(&#34;category_name&#34;, as_index=False)
    .agg({&#34;total_orders_with_lemma&#34;: &#34;max&#34;})
    .rename(columns={&#34;total_orders_with_lemma&#34;: &#34;max_lemma_count&#34;})
)

categories_with_max = categories_with_lemma_counts.merge(
    max_lemma_count,
    on=&#34;category_name&#34;
)

categories_mapping = categories_with_max[
    categories_with_max[&#39;total_orders_with_lemma&#39;] == categories_with_max[&#39;max_lemma_count&#39;]
].groupby(&#34;category_name&#34;, as_index=False).max()
categories_mapping.to_pandas()
</code></pre>
<p>Now that there is a single lemma summarizing each category, merge this to the original DataFrame.</p>
<pre><code>df_with_lemma = df.merge(
    categories_mapping,
    on=&#34;category_name&#34;,
    how=&#34;left&#34;
)
</code></pre>
<h2 is-upgraded>Comparing counties</h2>
<p>Compare sales in each county to see what differences there are.</p>
<pre><code>county_lemma = (
    df_with_lemma
    .groupby([&#34;county&#34;, &#34;lemma&#34;])
    .agg({&#34;volume_sold_liters&#34;: &#34;sum&#34;})
    # Cast to an integer for more deterministic equality comparisons.
    .assign(volume_sold_int64=lambda _: _[&#39;volume_sold_liters&#39;].astype(&#34;Int64&#34;))
)
</code></pre>
<p>Find the most sold product (lemma) in each county.</p>
<pre><code>county_max = (
    county_lemma
    .reset_index(drop=False)
    .groupby(&#34;county&#34;)
    .agg({&#34;volume_sold_int64&#34;: &#34;max&#34;})
)

county_max_lemma = county_lemma[
    county_lemma[&#34;volume_sold_int64&#34;] == county_max[&#34;volume_sold_int64&#34;]
]

county_max_lemma.to_pandas()
</code></pre>
<p>How different are the counties from each other?</p>
<pre><code>county_max_lemma.groupby(&#34;lemma&#34;).size().to_pandas()
</code></pre>
<p>In most counties, whiskey is the most popular product by volume, with vodka most popular in 15 counties. Compare this to the most popular liquor types statewide.</p>
<pre><code>total_liters = (
    df_with_lemma
    .groupby(&#34;lemma&#34;)
    .agg({&#34;volume_sold_liters&#34;: &#34;sum&#34;})
    .sort_values(&#34;volume_sold_liters&#34;, ascending=False)
)
total_liters.to_pandas()
</code></pre>
<p>Whiskey and vodka have nearly the same volume, with vodka a bit higher than whiskey statewide.</p>
<h2 is-upgraded>Comparing proportions</h2>
<p>What is unique about the sales in each county? What makes the county different from the rest of the state?</p>
<p>Use the <a href="https://en.wikipedia.org/wiki/Cohen%27s_h" target="_blank">Cohen&#39;s h measure</a> to find which liquor sales volumes differ the most proportionally from what would be expected based on the proportion of sales statewide.</p>
<pre><code>import numpy as np

total_proportions = total_liters / total_liters.sum()
total_phi = 2 * np.arcsin(np.sqrt(total_proportions))

county_liters = df_with_lemma.groupby([&#34;county&#34;, &#34;lemma&#34;]).agg({&#34;volume_sold_liters&#34;: &#34;sum&#34;})
county_totals = df_with_lemma.groupby([&#34;county&#34;]).agg({&#34;volume_sold_liters&#34;: &#34;sum&#34;})
county_proportions = county_liters / county_totals
county_phi = 2 * np.arcsin(np.sqrt(county_proportions))

cohens_h = (
    (county_phi - total_phi)
    .rename(columns={&#34;volume_sold_liters&#34;: &#34;cohens_h&#34;})
    .assign(cohens_h_int=lambda _: (_[&#39;cohens_h&#39;] * 1_000_000).astype(&#34;Int64&#34;))
)
</code></pre>
<p>Now that the Cohen&#39;s h has been measured for each lemma, find the largest difference from the statewide proportion in each county.</p>
<pre><code># Note: one might want to use the absolute value here if interested in counties
# that drink _less_ of a particular liquor than expected.
largest_per_county = cohens_h.groupby(&#34;county&#34;).agg({&#34;cohens_h_int&#34;: &#34;max&#34;})
counties = cohens_h[cohens_h[&#39;cohens_h_int&#39;] == largest_per_county[&#34;cohens_h_int&#34;]]
counties.to_pandas()
</code></pre>
<h2 is-upgraded>Visualizing counties</h2>
<p>Join with the <a href="https://console.cloud.google.com/bigquery?ws=!1m5!1m4!4m3!1sbigquery-public-data!2sgeo_us_boundaries!3scounties" target="_blank"><code>bigquery-public-data.geo_us_boundaries.counties</code> table</a> to get the geographic area for each county. County names are not unique across the United States, so filter to only include counties from Iowa. The FIPS code for Iowa is â€˜19&#39;.</p>
<pre><code>counties_geo = (
    bpd.read_gbq(&#34;bigquery-public-data.geo_us_boundaries.counties&#34;)
    .assign(county=lambda _: _[&#39;county_name&#39;].str.upper())
)
counties_plus = (
    counties
    .reset_index(drop=False)
    .merge(counties_geo[counties_geo[&#39;state_fips_code&#39;] == &#39;19&#39;], on=&#34;county&#34;, how=&#34;left&#34;)
    .dropna(subset=[&#34;county_geom&#34;])
    .to_pandas()
)
counties_plus
</code></pre>
<p>Use GeoPandas to visualize these differences on a map.</p>
<pre><code>import geopandas

counties_plus = geopandas.GeoDataFrame(counties_plus, geometry=&#34;county_geom&#34;)

# https://stackoverflow.com/a/42214156/101923
ax = counties_plus.plot(figsize=(14, 14))
counties_plus.apply(
    lambda row: ax.annotate(
        text=row[&#39;lemma&#39;],
        xy=row[&#39;county_geom&#39;].centroid.coords[0],
        ha=&#39;center&#39;
    ),
    axis=1,
)
</code></pre>
<p class="image-container"><img alt="A map of the alcohol that is most different from statewide sales volume proportions in each county" src="img/c8125442bea4ebc9.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Clean up" duration="0">
        <p>If you have created a new Google Cloud project for this tutorial, you can <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects" target="_blank">delete it</a> to prevent additional charges for tables or other resources created.</p>
<p>Alternatively, delete the Cloud Functions, service accounts, and datasets created for this tutorial.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations!" duration="0">
        <p>You have analyzed structured and unstructured data using BigQuery DataFrames. Along the way you&#39;ve explored Google Cloud&#39;s Public Datasets, Python notebooks in BigQuery Studio, BigQuery ML, Vertex AI, and natural language to Python features of BigQuery Studio. Fantastic job!</p>
<h2 is-upgraded>Next steps</h2>
<ul>
<li>Apply these steps to other data, such as the <a href="https://github.com/tswast/code-snippets/blob/main/2024/12-bigframes-usa-names/usa_names.ipynb" target="_blank">USA names database</a>.</li>
<li>Try <a href="https://cloud.google.com/colab/docs/use-code-completion" target="_blank">generating Python code in your notebook</a>. Python notebooks in BigQuery Studio are powered by Colab Enterprise. Hint: I find asking for help generating test data to be quite useful.</li>
<li>Explore the <a href="https://github.com/googleapis/python-bigquery-dataframes/tree/main/notebooks" target="_blank">sample notebooks for BigQuery DataFrames</a> on GitHub.</li>
<li>Create a <a href="https://cloud.google.com/bigquery/docs/orchestrate-notebooks" target="_blank">schedule to run a notebook in BigQuery Studio</a>.</li>
<li>Deploy a <a href="https://cloud.google.com/bigquery/docs/samples/bigquery-dataframes-remote-function" target="_blank">Remote Function with BigQuery DataFrames</a> to integrate third-party Python packages with BigQuery.</li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
